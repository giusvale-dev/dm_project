\documentclass[conference]{IEEEtran}

\usepackage{graphicx} % For including images
\usepackage{amsmath} % For mathematical symbols and equations
\usepackage{hyperref} % For hyperlinks
\usepackage{caption}
\usepackage{amsfonts} % For math fonts and symbols
\usepackage{booktabs} % For better tables
\usepackage{listings} % For code listings
\usepackage{multirow} % For multi-row cells in tables

\begin{document}
	
	\title{Data Management \\ Project of a Data Warehouse System for Analyzing Meteorite Landings by NASA}
	
	\author{
		\IEEEauthorblockN{Giuseppe Valente - Natalia Maria Mucha}
		\IEEEauthorblockA{
			Engineering in Computer Science \\
			Sapienza University of Rome \\
		}
	}
	\maketitle
	
	\begin{abstract}
			This project focuses on developing a data warehouse system to integrate and analyze meteorite landing data from NASA. The objective is to demonstrate how to design and implement a data warehouse on a relational database using ETL processes, a dimensional fact model (DFM) schema, and Postgres SQL for deployment. 
	\end{abstract}
	
	\section{Introduction}
	
	This project aims to develop a data warehouse system specifically designed to integrate and analyze data on meteorite landings using NASA's Meteorite Landings dataset. The projectâ€™s objective is to demonstrate how to design and implement a data warehouse on a relational database platform using Postgres SQL. The process involves the application of Extract, Transform, Load (ETL) techniques to systematically collect, clean, and organize the data. Additionally, the project employs a Dimensional Fact Model (DFM) schema to structure the data in a way that supports in-depth analysis. \\ The result is a powerful tool for advancing the study of meteorites and their impacts on Earth.
	
	\section{Data warehouse}
	A data warehouse (DWH), is a system used for reporting and make data analysis. Is a central repository of integrated data from one or more sources (dataset or data source). They store current and historical data in one single place that are used for creating reports. This is beneficial for companies as it enables them to interrogate and draw insights from their data and make decisions.\\ The data stored in the DWH is uploaded from operational systems but before pass through a process to ensure data quality before to use the DWH analysis (ETL process).\\ The DWH is a multidimensional model represented in \textbf{facts} that are the subject of analysis represented as cubes, where:
	\begin{itemize}
		\item Each cell contains numerical \textbf{measures} (quantifing the fact from one point of view)
		\item Each axis represents a \textbf{dimension} of interest for analysis
		\item Each dimension can be associated with a \textbf{hierarchy of dimensional attributes} used to aggregate data stored into cubes.
	\end{itemize}
	To read information in the DWH are used four methodologies:
	\begin{itemize}
		\item Reports
		\item Dashboards
		\item Data mining
		\item OLAP
	\end{itemize}
	 
	\section{Requirements}
	We are interesting to make analisys on meterotite landings, for this reason we use the following information:
	\begin{itemize}
		\item \textbf{Measure:} Meteorite mass in grams 
		\item \textbf{Dimensions:} Time, Location, Meteorite Classification
	\end{itemize}
	
	\section{Dimensional Fact Model (DFM)}
	\begin{figure}[htpb]
		\centering
		\includegraphics[width=\columnwidth]{images/dfm_schema.png}
		\caption{Meteorite Landings System Architecture}
		\label{fig:Meteorite Landings System Architecture}
	\end{figure}
	To design a DWH system the DFM schema is required and through it we can model facts, measures, dimensions and hierarchies. \\
	\section{Star Schema}
	A multidimensional structure can be represented using two distinct logical models:
	\begin{itemize}
		\item MOLAP (Multidimensional On-Line Analytical Processing)
		\item ROLAP (Relational On-Line Analytical Processing)
	\end{itemize}
	For our purpose we use the ROLAP model using the start schema. Therefore we can design the star schema for our DFM model:
	
	\section{System Architecture}
	The DWH architecture in general is composed by the following components:
	\begin{itemize}
		\item \textbf{Datasets:} are collections of structured data organized to support reporting, analysis, and decision-making. These datasets are derived from various sources and are typically organized in a way that optimizes them for querying and analysis. 
		\item \textbf{ETL:} stands for Extract, Transform, Load, which is a process used in data warehousing to move data from source systems to a data warehouse.
		\item \textbf{Primary DWH:} refers to the central, most important repository where an organization consolidates, stores, and manages its key business data for analysis and reporting. It is designed to support decision-making processes by providing a comprehensive view of data from various sources.
		\item \textbf{Data mart:} is a specialized subset of a DWH, designed to focus on a specific business area, department, or function. It is essentially a smaller, more focused version of a DWH that serves the needs of particular users or business units. 
		\item \textbf{Reconcilied data (three layer architecture only):} refers to data that has been verified, adjusted, and aligned to ensure consistency, accuracy, and completeness across various sources or systems. The process of reconciling data involves comparing and aligning data from different sources to resolve discrepancies and ensure that the data is accurate and reliable.
	\end{itemize}
	
	The components are organized in a two or three layers architecture: 	
	\subsection{Two-Layer Architecture (2LA)}
	The 2LA is an architecture composed by two mainly elements, the sources and the DWH. In this architecture the data marts are considerated as the "presentation layer" of the architecture. This architecture is very useful in organization from mid to large size. \\
	\subsection{Three-Layer Architecture (3LA)}
	In this architecture there is an additional layer, the reconcilied data layer, that it creates a common reference data model for a whole enterprise. At the same time, it sharply separates the problems of source data extraction and integration from those of DWH population. However the reconciled data leads to more replication of operational source data and the	design process is complicated. \\
			
	\subsection{Meteorite Landings - Architecture}
	We consider that, for our purposes, it is better to use the 2LA because we do not have many data sources, the ETL process is simple, and it can be managed on the fly. However, it is important to consider that in a real scenario, a good choice would be to use a 3LA. With this architecture, it is possible to separate Data Staging from Data Loading through the Reconciled Data Layer. This allows for the design of simpler and more efficient ETL tools, each focused on solving one specific task (e.g., \texttt{extract.py}, \texttt{transform.py}, \texttt{load.py}). Additionally, using containerized technologies like Kubernetes or Docker can enable horizontal scaling of these tools, allowing the system to be reactive in case of increased computational load or growth in the data sources.\\ For simplicity, we use Docker to distribute the application in the simplest way possible, but this does not mean that the application is scalable.\\
	As shown in Figure~\ref{fig:Meteorite Landings System Architecture}, we have the following components in our Architecture:
	\begin{itemize}
		\item \textbf{Datasets: (Meteorite Landings, Meteoritical Bulletin, Socrata Location)}
		\item \textbf{ETL: (meteorite\_landings\_etl)}
		\item \textbf{Primary DWH: PostgreSQL}
		\item \textbf{Data marts: out of scope}  
	\end{itemize}
	\begin{figure}[htpb]
		\centering
		\includegraphics[width=\columnwidth]{images/system_architecture.png}
		\caption{Meteorite Landings System Architecture}
		\label{fig:Meteorite Landings System Architecture}
	\end{figure}
	
	\section{Source Layer}
	
	\subsection{Meteorite Landings Dataset}
	\label{subsec:meteorite landings data set}
	This comprehensive data set from The Meteoritical Society contains information on all of the known meteorite landings. The Fusion Table is collected by Javier de la Torre and we've also provided an XLS file that consists of 34.513 meteorites. \\
	\href{https://data.nasa.gov/Space-Science/Meteorite-Landings/gh4g-9sfh/about_data}{NASA's Open Data Portal}
	
	\subsection{Meteoritical Bulletin Database}
	This database has been constructed and is maintained by the Nomenclature Committee of the Meteoritical Society. The primary function of this database is to provide authoritative information about meteorite names. The correct spelling, complete with punctuation and diacritical marks, of all known meteorites recognized by the Meteoritical Society may be found in this compilation. Official abbreviations for many meteorites are documented here as well. The database also contains status information for meteorites with provisional names, and listings for specimens of doubtful origin and pseudometeorites. \\
	\href{https://www.lpi.usra.edu/meteor/}{Lunar and Planetary Institute}
	
	\subsection{Socrata Location}
	Dataset used to revert the coordinate point from latitude,longitude to human readable format.
	\href{https://dev.socrata.com/docs/datatypes/location.html#}{Tyler Technologies - Socdata}
				
	\section{ETL tools}
	The ETL process is achieved using \textbf{meteorite\_landings\_etl}.\\ It is a software developed in Python containing the following modules:
	\begin{itemize}
		\item \textbf{extract} - Reads the data from the dataset \ref{subsec:meteorite landings data set}
		\item \textbf{transform} - Reads the data rows from the extract module and therefore it is responsible to make cleaning and trasformation the data, using the other datasets and the information meta-data.
		\item \textbf{load} - Reads the row data updated by the trasform module, then load them in the DWH database 
	\end{itemize}
	The details are described in the following sections.
	\subsection{Extract}
    Extraction is the process that allows us to obtain data from sources, and we know that this operation can be either static (generally used when the DWH needs to be populated for the first time) or incremental (used to update the DWH regularly). Since our datasets are not updated at regular intervals, we have decided to implement only static extraction. If an update is required, we will back up the previous DWH and then populate a new DWH system.\\ The extraction will return the following data rows:
    
	
	\subsection{Transform}
	The transformation process includes converting geo-coordinates to human-readable formats and normalizing data formats across all datasets.
	
	\subsection{Load}
	The transformed data is loaded into the relational database using SQLAlchemy in Python, ensuring that all datasets are correctly integrated for analysis.
	
	\section{Data warehouse Layer}
	
	\subsection{Dimensions Fact Model (DFM)}
	
	\subsection{Relational Database realization}

	\section{Conclusion}
	
	\section{Reference}
	\begin{itemize}
		\item \href{https://web.pdx.edu/~ruzickaa/meteorites/papers/WeisbergEtal2006-classification.pdf}{Weisberg et al. (2006) Systematics and Evaluation of Meteorite Classification. In, Meteorites and the Early Solar System II, 19-52 (D.S. Lauretta and H.Y. McSween, Eds.), Univ. Arizona press}
		 
	\end{itemize}
	
	
	
	
	
\end{document}
